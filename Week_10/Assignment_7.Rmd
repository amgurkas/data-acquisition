---
title: "Insert Here"
author: "Alyssa Gurkas"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    number_sections: false
    toc_collapsed: true
    toc_depth: 4
---

[Github Repository](https://github.com/amgurkas/data-acquisition/tree/main/Week_10/) 

```{r set-chunk-options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,      
  results = "hide", 
  warning = FALSE,  
  error = FALSE,   
  message = FALSE)
```

### Load Libraries
```{r load-library}
library(tidyverse)
library(httr)
library(jsonlite)
library(sentimentr)
```

### Connecting to Twitter API 
```{r twitter-api}
# Retrieve bearer token from environment
bearer_token <- Sys.getenv("BEARER_TOKEN")
# Define token header and paste together (sprintf similar to paste0)
headers <- c(`Authorization` = sprintf('Bearer %s', bearer_token))
# Define parameters where max results is 500, querying for the string tariff, and then sorting results by relevance
params <- list(
    max_results = 100,
    query = "tariffs",
    sort_order = "relevancy"
    
)
# defining url_handle for the recent tweets url
url_handle <- "https://api.twitter.com/2/tweets/search/recent"
# saving the response object 
res <-
  httr::GET(url = url_handle,
            httr::add_headers(.headers = headers),
            query = params)

# Check response type
http_type(res)
# Check for errors
http_error(res)

# parsing the json
recent_search_body <-
  content(
    res,
    as = 'parsed',
    type = 'application/json',
    simplifyDataFrame = TRUE
  )

recent_text_body <- recent_search_body$data
```
### Loading the Tweet Eval Model
```{r}
# Retrieve bearer token from environment for HF Model 
hf_bearer_token <- Sys.getenv("HF_TOKEN")

get_sentiment <- function(tweet){
# Get sentiment predictions
  sent_res <- POST(
    url = "https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment",
    add_headers("Authorization" = paste("Bearer", hf_bearer_token)),
    content_type("application/json"),
    body = toJSON(
      list(
        inputs = tweet
      ),
      auto_unbox = TRUE
    )
  )
  
sent_content <- fromJSON(rawToChar(sent_res$content))
}

tweet <- "You know who doesn't have to pay tariffs"
sentiment_result <- get_sentiment(tweet)

```

### Data Cleaning
```{r}
clean_text = function(rawtext)
{
  # convert to lower case
  rawtext = tolower(rawtext)
  # remove rt
  rawtext = gsub("rt", "", rawtext)
  # remove at
  rawtext = gsub("@\\w+", "", rawtext)
  # remove punctuation
  rawtext = gsub("[[:punct:]]", "", rawtext)
  # remove numbers
  rawtext = gsub("[[:digit:]]", "", rawtext)
  # remove links http
  rawtext = gsub("http\\w+", "", rawtext)
  # remove tabs
  rawtext = gsub("[ |\t]{2,}", "", rawtext)
  # remove blank spaces at the beginning
  rawtext = gsub("^ ", "", rawtext)
  # remove blank spaces at the end
  rawtext = gsub(" $", "", rawtext)
  # some other cleaning text
  rawtext = gsub('https://','',rawtext)
  rawtext = gsub('http://','',rawtext)
  rawtext = gsub('[^[:graph:]]', ' ',rawtext)
  rawtext = gsub('[[:punct:]]', '', rawtext)
  rawtext = gsub('[[:cntrl:]]', '', rawtext)
  rawtext = gsub('\\d+', '', rawtext)
  rawtext = str_replace_all(rawtext,"[^[:graph:]]", " ")
  return(rawtext)
}

cleaned_recent_text <- recent_text_body |> 
  mutate(clean_text = clean_text(recent_text_body$text))
```

### Sentiment Exploratory Analysis with `sentimentr` package
```{r}
# Example with one tweet
tweet <- "You know who doesn't have to pay tariffs because they manufacture everything in the United States. Hollywood Product"
sentiment_result <- get_sentiment(tweet)
sentiment_r_package <- sentiment(tweet)
sentiment_all_tweets <- sentiment(cleaned_recent_text$clean_text)

summary <- sentiment_all_tweets |> 
  summarise(
    average_weighted_mixed_sentiment = average_weighted_mixed_sentiment(sentiment),
    average_mean = average_mean(sentiment),
    average = mean(sentiment),
    sd = sd(sentiment),
    iqr = IQR(sentiment)
  )

ggplot(data = sentiment_all_tweets, aes(x = element_id, 
                            y = sentiment,
                            color = "Tweets"))+
    geom_point(alpha = .4)+
    scale_color_manual(name = "", values=c("#FF6885", "white"))+
    geom_smooth(se=FALSE)+
    xlab("Sentence Order")+
    ylab("Sentiment")

# not enough CPU/RAM to use the ai tool
# sentiment_result_ai <- sentiment_score(tweet)
```

### Connecting to Sentiment Analysis Model 
```{r}


cleaned_recent_text$text <- lapply(cleaned_recent_text$text, function(x))
   get_sentiment(x), error = function(e) list(negative = NA, neutral = NA, positive = NA))
 })

# Extract scores to separate columns
tweets_df <- cbind(tweets_df, do.call(rbind, tweets_df$sentiment))

```



